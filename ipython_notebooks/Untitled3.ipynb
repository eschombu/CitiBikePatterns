{
 "metadata": {
  "name": "",
  "signature": "sha256:56642a5f1c1ebb199171db18ba73d7df1c00e6c4725ea66e39d2cdf80ad4a1cc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy import stats\n",
      "import pandas as pd\n",
      "from pandas import Series, DataFrame\n",
      "from sklearn import linear_model\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "from datetime import date, datetime, timedelta\n",
      "import datetime as dt\n",
      "from pytz import timezone\n",
      "import pytz\n",
      "import time\n",
      "import calendar\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sbs\n",
      "from matplotlib import gridspec\n",
      "\n",
      "## Function returning list of boolean values indicating which elements of A are in B\n",
      "def ismember(A, B):\n",
      "    return [ bool(np.sum(a == B)) for a in A ]\n",
      "\n",
      "\n",
      "## Rounding time, from http://stackoverflow.com/questions/3463930/how-to-round-the-minute-...\n",
      "## ...of-a-datetime-object-python/10854034#10854034\n",
      "def roundTime(dt=None, roundTo=60):\n",
      "   \"\"\"Round a datetime object to any time laps in seconds\n",
      "   dt : datetime.datetime object, default now.\n",
      "   roundTo : Closest number of seconds to round to, default 1 minute.\n",
      "   Author: Thierry Husson 2012 - Use it as you want but don't blame me.\n",
      "   \"\"\"\n",
      "   if dt == None : dt = datetime.datetime.now()\n",
      "   seconds = (dt - dt.min).seconds\n",
      "   # // is a floor division, not a comment on following line:\n",
      "   rounding = (seconds+roundTo/2) // roundTo * roundTo\n",
      "   return dt + datetime.timedelta(0,rounding-seconds,-dt.microsecond)\n",
      "\n",
      "\n",
      "## Hand-set holidays that I think typically affect travel\n",
      "file = open('/home/vagrant/citibike/holidays.txt', 'r')\n",
      "holiday_strs = []\n",
      "for line in file:\n",
      "    holiday_strs.append(line.rstrip())\n",
      "\n",
      "file.close()\n",
      "file = open('/home/vagrant/citibike/holiday_related.txt', 'r')\n",
      "holidayRelated_strs = []\n",
      "for line in file:\n",
      "    holidayRelated_strs.append(line.rstrip())\n",
      "\n",
      "file.close()\n",
      "holidays = [datetime.strptime(x, '%Y-%m-%d').date() for x in holiday_strs]\n",
      "holidayRelated = [datetime.strptime(x, '%Y-%m-%d').date() for x in holidayRelated_strs]\n",
      "\n",
      "## Load Citi Bike data\n",
      "dtfmt = \"%Y-%m-%d %H:%M:%S\"\n",
      "CBmonths = np.append(np.append(np.append(np.ones((5,1))*2013, np.ones((12,1))*2014), np.ones((2,1))*2015), \\\n",
      "    np.array(range(8,13,1) + range(1,13,1) + range(1,3,1))).astype('int').reshape(2,19).T\n",
      "mo_strs = []\n",
      "for i in range(CBmonths.shape[0]):\n",
      "    mo_strs.append(date(CBmonths[i,0], CBmonths[i,1], 1))\n",
      "\n",
      "citibike_filename = \"/home/vagrant/citibike/hourlyDB_byStart_\" + mo_strs[0].strftime(\"%m-%Y\") + \\\n",
      "    \"_to_\" + mo_strs[-1].strftime(\"%m-%Y\") + \".csv.gz\"\n",
      "hourlyDF_bystart = pd.read_csv(citibike_filename, compression=\"gzip\")\n",
      "hourlyDF_bystart['start time'] = [pytz.timezone('US/Eastern').localize(\\\n",
      "    datetime(*datetime.strptime(x, dtfmt).timetuple()[0:4], \\\n",
      "    minute=int(np.floor(datetime.strptime(x, dtfmt).minute/30.)*30))) \\\n",
      "    for x in hourlyDF_bystart['start time'].values]\n",
      "\n",
      "reftime = time.mktime(time.strptime(\"2000-01-01 00:00:00\", \"%Y-%m-%d %H:%M:%S\"))\n",
      "binstart = time.mktime(time.strptime(date(CBmonths[0,0], CBmonths[0,1], 1 \\\n",
      "    ).strftime(\"%Y-%m-%d %H:%M:%S\"), \"%Y-%m-%d %H:%M:%S\"))\n",
      "binend = time.mktime(time.strptime(date(CBmonths[-1,0], CBmonths[-1,1] + 1, 1 \\\n",
      "    ).strftime(\"%Y-%m-%d %H:%M:%S\"), \"%Y-%m-%d %H:%M:%S\"))\n",
      "hourbins = np.linspace(binstart-reftime, binend-reftime, (binend-binstart)/3600 + 1)\n",
      "hourbins_dt = [datetime.fromtimestamp(x + reftime) for x in hourbins]\n",
      "\n",
      "## Load weather data\n",
      "start_weather = date(2013,7,1)\n",
      "end_weather = date(2014,8,31)\n",
      "weather_filename = \"/home/vagrant/citibike/weatherDB_\" + start_weather.strftime(\"%Y-%m-%d\") \\\n",
      "    + \"_to_\" + end_weather.strftime(\"%Y-%m-%d\") + \".csv\"\n",
      "weather_df = pd.read_csv(weather_filename, index_col=0)\n",
      "\n",
      "## Index by proper UTC DateTimes, using nearest previous 1/2 hours (same as Citi Bike)\n",
      "weather_df[\"DateTime_UTC\"] = [datetime(*datetime.strptime(x, dtfmt).timetuple()[0:4], tzinfo=pytz.utc) \\\n",
      "    + timedelta(seconds=round(datetime.strptime(x, dtfmt).minute/60.)*3600) for x in weather_df[\"DateUTC\"].values]\n",
      "weather_df = weather_df.groupby('DateTime_UTC', as_index=False).first()\n",
      "weather_df[\"start time\"] = weather_df[\"DateTime_UTC\"].map(lambda x: \\\n",
      "    pytz.timezone('UTC').localize(x).astimezone(pytz.timezone('US/Eastern')))\n",
      "\n",
      "## Get rid of bad temperature values\n",
      "weather_df[\"TemperatureF\"].ix[weather_df[\"TemperatureF\"] < -100] = np.nan\n",
      "\n",
      "def condition_number_map(conditions):\n",
      "    return {\n",
      "        'Drizzle': 1, 'Rain': 1, 'Snow': 1, 'Snow Grains': 1, 'Ice Crystals': 1, \\\n",
      "        'Ice Pellets': 1, 'Hail': 1, 'Mist': 1, 'Fog': 0, 'Fog Patches': 0.5, 'Smoke': 0, \\\n",
      "        'Volcanic Ash': 0, 'Widespread Dust': 0, 'Sand': 0, 'Haze': 0.5, 'Spray': 0.5, \\\n",
      "        'Dust Whirls': 0.5, 'Sandstorm': 0, 'Low Drifting Snow': 0.5, \\\n",
      "        'Low Drifting Widespread Dust': 0.5, 'Low Drifting Sand': 0.5, 'Blowing Snow': 0.5, \\\n",
      "        'Blowing Widespread Dust': 0.5, 'Blowing Sand': 0.5, 'Rain Mist': 1, 'Rain Showers': 1, \\\n",
      "        'Snow Showers': 1, 'Snow Blowing Snow Mist': 0.5, 'Ice Pellet Showers': 1, 'Hail Showers': 1, \\\n",
      "        'Small Hail Showers': 1, 'Thunderstorm': 0.5, 'Thunderstorms and Rain': 0.5, \\\n",
      "        'Thunderstorms and Snow': 0.5, 'Thunderstorms and Ice Pellets': 0.5, 'Thunderstorms with Hail': 0.5, \\\n",
      "        'Freezing Fog': 0, 'Thunderstorms with Small Hail': 0.5, 'Freezing Drizzle': 1, \\\n",
      "        'Freezing Rain': 1, 'Patches of Fog': 0.5, 'Shallow Fog': 0.5, 'Partial Fog': 0.5, \\\n",
      "        'Overcast': 1, 'Clear': 5, 'Partly Cloudy': 3, 'Mostly Cloudy': 4, 'Scattered Clouds': 2, \\\n",
      "        'Small Hail': 1, 'Squalls': 1, 'Funnel Cloud': 0, 'Unknown Precipitation': 1, 'Unknown': np.nan\n",
      "    }[conditions]\n",
      "\n",
      "hourly_gp = hourlyDF_bystart.groupby(\"start time\")\n",
      "hourly_df = DataFrame({\"start time\":hourly_gp[\"start time\"].first(), \"count\":hourly_gp[\"total count\"].sum().values})\n",
      "hourly_df[\"weekday\"] = hourly_df[\"start time\"].map(lambda x: calendar.weekday(x.year, x.month, x.day))\n",
      "hourly_df[\"hour\"] = hourly_df[\"start time\"].map(lambda x: x.hour)\n",
      "hourly_df[\"HOLIDAY\"] = hourly_df[\"start time\"].map(lambda x: x.date() in holidays+holidayRelated)\n",
      "hourly_df = hourly_df.merge(weather_df[[\"start time\", \"TemperatureF\", \"PrecipitationIn\", \\\n",
      "    \"Humidity\", \"Wind SpeedMPH\", \"Conditions\"]], how='left', on=\"start time\")\n",
      "hourly_df[\"PrecipitationIn\"] = hourly_df[\"PrecipitationIn\"].fillna(0)\n",
      "hourly_df[\"Wind SpeedMPH\"].ix[hourly_df[\"Wind SpeedMPH\"] == '-9999.0'] = 'nan'\n",
      "hourly_df[\"Wind SpeedMPH\"].ix[hourly_df[\"Wind SpeedMPH\"] == 'Calm'] = '0'\n",
      "hourly_df[\"Wind SpeedMPH\"] = hourly_df[\"Wind SpeedMPH\"].map(float)\n",
      "hourly_df['Conditions'].ix[hourly_df['Conditions'].isnull()] = 'Unknown'\n",
      "hourly_df[\"Condition index\"] = hourly_df[\"Conditions\"].map(lambda x: \\\n",
      "    condition_number_map(x.replace('Light ', '').replace('Heavy ', '')))\n",
      "\n",
      "\n",
      "## For time points where Temperature not available, interpolate values\n",
      "null_inds = hourly_df.index[hourly_df['TemperatureF'].isnull()].values\n",
      "not_null = hourly_df.index[~hourly_df['TemperatureF'].isnull()].values\n",
      "hourly_df['TemperatureF'].ix[null_inds] = np.interp(null_inds, not_null, hourly_df['TemperatureF'].ix[not_null].values)\n",
      "\n",
      "\n",
      "## Create feature vectors\n",
      "def make_features(train_df, trainOn_bools):\n",
      "    \n",
      "    # Current precip\n",
      "    current_precip = train_df[\"PrecipitationIn\"].ix[trainOn_bools].values.reshape(-1, 1)\n",
      "    \n",
      "    # Total precip in previous 12, 24, 48 hours, and next 12 hours\n",
      "    prev12_precip = np.zeros([np.sum(trainOn_bools), 12])\n",
      "    for i in range(12):\n",
      "        shifted_bools = np.append(trainOn_bools[i+1:], np.array([False for n in range(i+1)])).astype('bool')\n",
      "        prev12_precip[:,i] = train_df[\"PrecipitationIn\"].ix[shifted_bools].values\n",
      "    \n",
      "    prev12_precip = np.sum(prev12_precip, axis=1).reshape(-1, 1)\n",
      "    \n",
      "    prev24_precip = np.zeros([np.sum(trainOn_bools), 12])\n",
      "    for i in range(12):\n",
      "        shifted_bools = np.append(trainOn_bools[i+13:], np.array([False for n in range(i+13)])).astype('bool')\n",
      "        prev24_precip[:,i] = train_df[\"PrecipitationIn\"].ix[shifted_bools].values\n",
      "    \n",
      "    prev24_precip = prev12_precip + np.sum(prev24_precip, axis=1).reshape(-1, 1)\n",
      "    \n",
      "    prev48_precip = np.zeros([np.sum(trainOn_bools), 24])\n",
      "    for i in range(24):\n",
      "        shifted_bools = np.append(trainOn_bools[i+25:], np.array([False for n in range(i+25)])).astype('bool')\n",
      "        prev48_precip[:,i] = train_df[\"PrecipitationIn\"].ix[shifted_bools].values\n",
      "    \n",
      "    prev48_precip = prev24_precip + np.sum(prev48_precip, axis=1).reshape(-1, 1)\n",
      "    \n",
      "    prev72_precip = np.zeros([np.sum(trainOn_bools), 24])\n",
      "    for i in range(24):\n",
      "        shifted_bools = np.append(trainOn_bools[i+25:], np.array([False for n in range(i+25)])).astype('bool')\n",
      "        prev72_precip[:,i] = train_df[\"PrecipitationIn\"].ix[shifted_bools].values\n",
      "    \n",
      "    prev72_precip = prev48_precip + np.sum(prev72_precip, axis=1).reshape(-1, 1)\n",
      "    \n",
      "    next6_precip = np.zeros([np.sum(trainOn_bools), 6])\n",
      "    for i in range(1,7):\n",
      "        shifted_bools = np.append(np.array([False for n in range(i)]), trainOn_bools[:-i]).astype('bool')\n",
      "        next6_precip[:,i-1] = train_df[\"PrecipitationIn\"].ix[shifted_bools].values\n",
      "    \n",
      "    next6_precip = np.sum(next6_precip, axis=1).reshape(-1, 1)\n",
      "    \n",
      "    # Current temp, and high & low in +/- 12 hours\n",
      "    current_temp = train_df[\"TemperatureF\"].ix[trainOn_bools].values.reshape(-1, 1)\n",
      "    \n",
      "    prev12_temp = np.zeros([np.sum(trainOn_bools), 12])\n",
      "    for i in range(12):\n",
      "        shifted_bools = np.append(trainOn_bools[i+1:], np.array([False for n in range(i+1)])).astype('bool')\n",
      "        prev12_temp[:,i] = train_df[\"TemperatureF\"].ix[shifted_bools].values\n",
      "        \n",
      "    next12_temp = np.zeros([np.sum(trainOn_bools), 12])\n",
      "    for i in range(1,13):\n",
      "        shifted_bools = np.append(np.array([False for n in range(i)]), trainOn_bools[:-i]).astype('bool')\n",
      "        next12_temp[:,i-1] = train_df[\"TemperatureF\"].ix[shifted_bools].values\n",
      "    \n",
      "    hi24_temp = np.max(np.concatenate((prev12_temp, next12_temp), axis=1), axis=1).reshape(-1, 1)\n",
      "    lo24_temp = np.min(np.concatenate((prev12_temp, next12_temp), axis=1), axis=1).reshape(-1, 1)\n",
      "    mean24_temp = np.mean(np.concatenate((current_temp, prev12_temp, next12_temp), axis=1), axis=1).reshape(-1, 1)\n",
      "    \n",
      "    # Interactions\n",
      "    polyFeat = PolynomialFeatures(degree=3, include_bias=True)\n",
      "    features = polyFeat.fit_transform(np.concatenate((current_precip, prev48_precip, \\\n",
      "        current_temp, mean24_temp), axis=1))\n",
      "    \n",
      "    # Add constant\n",
      "    features = np.concatenate((np.ones([np.sum(trainOn_bools),1]), features), axis=1)\n",
      "    \n",
      "    return features   #--- make_features() ---\n",
      "\n",
      "\n",
      "def model_quality(prediction, actual):\n",
      "    return np.sum(np.abs(prediction - actual))\n",
      "\n",
      "\n",
      "train_days = [min(hourly_df['start time']).date() + timedelta(hours=48) + timedelta(days=x) for x in \\\n",
      "    range(0, (max(hourly_df['start time']).date() - min(hourly_df['start time']).date() - timedelta(hours=60)).days, 2)]\n",
      "test_days = [min(hourly_df['start time']).date() + timedelta(hours=48) + timedelta(days=x) for x in \\\n",
      "    range(1, (max(hourly_df['start time']).date() - min(hourly_df['start time']).date() - timedelta(hours=60)).days, 2)]\n",
      "\n",
      "## convert to boolean index vectors\n",
      "wkday_train_bools = hourly_df['start time'].map(lambda x: x.date()).isin(train_days) & \\\n",
      "    hourly_df[\"weekday\"].isin(range(0,5))\n",
      "wkday_test_bools = hourly_df['start time'].map(lambda x: x.date()).isin(test_days) & \\\n",
      "    hourly_df[\"weekday\"].isin(range(0,5))\n",
      "wkend_train_bools = hourly_df['start time'].map(lambda x: x.date()).isin(train_days) & \\\n",
      "    hourly_df[\"weekday\"].isin(range(5,7))\n",
      "wkend_test_bools = hourly_df['start time'].map(lambda x: x.date()).isin(test_days) & \\\n",
      "    hourly_df[\"weekday\"].isin(range(5,7))\n",
      "\n",
      "\n",
      "## Fit model for each hour of day\n",
      "LOG = False\n",
      "wkday_models = []\n",
      "wkend_models = []\n",
      "for i in range(24):\n",
      "\n",
      "    # Weekdays\n",
      "    train_bools = wkday_train_bools & (hourly_df[\"hour\"].values == i)\n",
      "    train_features = make_features(hourly_df, train_bools)\n",
      "    train_notNA = ~np.isnan(train_features).any(axis=1)\n",
      "    if LOG:\n",
      "        train_responses = np.log10(hourly_df[\"count\"].ix[train_bools])\n",
      "    else:\n",
      "        train_responses = hourly_df[\"count\"].ix[train_bools]\n",
      "\n",
      "    #print \"i=\", i, \" weekday: Training model on feature matrix of size\", train_features[train_notNA,:].shape\n",
      "    #wkday_models.append( linear_model.LinearRegression().fit(train_features[train_notNA,:], \\\n",
      "    #    train_responses[train_notNA]) )\n",
      "    wkday_models.append( linear_model.RidgeCV(alphas=[0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0]).fit( \\\n",
      "        train_features[train_notNA,:], train_responses[train_notNA]) )\n",
      "    \n",
      "    # Weekends\n",
      "    train_bools = wkend_train_bools & (hourly_df[\"hour\"].values == i)\n",
      "    train_features = make_features(hourly_df, train_bools)\n",
      "    train_notNA = ~np.isnan(train_features).any(axis=1)\n",
      "    if LOG:\n",
      "        train_responses = np.log10(hourly_df[\"count\"].ix[train_bools])\n",
      "    else:\n",
      "        train_responses = hourly_df[\"count\"].ix[train_bools]\n",
      "\n",
      "    #print \"i=\", i, \" weekend: Training model on feature matrix of size\", train_features[train_notNA,:].shape\n",
      "    #wkend_models.append( linear_model.LinearRegression().fit(train_features[train_notNA,:], \\\n",
      "    #    train_responses[train_notNA]) )\n",
      "    wkend_models.append( linear_model.RidgeCV(alphas=[0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0]).fit( \\\n",
      "        train_features[train_notNA,:], train_responses[train_notNA]) )\n",
      "\n",
      "\n",
      "## Get model predictions and performances for training and test sets\n",
      "model_predictions = hourly_df[['start time', 'hour']].copy()\n",
      "model_predictions['count'] = np.zeros(hourly_df.index.values.shape)\n",
      "model_predictions['weekday'] = wkday_train_bools | wkday_test_bools\n",
      "model_predictions['weekend'] = wkend_train_bools | wkend_test_bools\n",
      "model_predictions['train'] = wkday_train_bools | wkend_train_bools\n",
      "model_predictions['test'] = wkday_test_bools | wkend_test_bools\n",
      "for i in range(24):\n",
      "    # Training weekdays\n",
      "    train_bools = wkday_train_bools & (hourly_df[\"hour\"].values == i)\n",
      "    train_features = make_features(hourly_df, train_bools)\n",
      "    train_notNA = ~np.isnan(train_features).any(axis=1)\n",
      "    train_predictions = np.zeros(np.sum(train_bools))\n",
      "    train_predictions.fill(np.nan)\n",
      "    if LOG:\n",
      "        train_predictions[train_notNA] = 10**wkday_models[i].predict(train_features[train_notNA])\n",
      "    else:\n",
      "        train_predictions[train_notNA] = wkday_models[i].predict(train_features[train_notNA])\n",
      "    model_predictions['count'].ix[train_bools] = train_predictions\n",
      "    \n",
      "    # Training weekends\n",
      "    train_bools = wkend_train_bools & (hourly_df[\"hour\"].values == i)\n",
      "    train_features = make_features(hourly_df, train_bools)\n",
      "    train_notNA = ~np.isnan(train_features).any(axis=1)\n",
      "    train_predictions = np.zeros(np.sum(train_bools))\n",
      "    train_predictions.fill(np.nan)\n",
      "    if LOG:\n",
      "        train_predictions[train_notNA] = 10**wkend_models[i].predict(train_features[train_notNA])\n",
      "    else:\n",
      "        train_predictions[train_notNA] = wkend_models[i].predict(train_features[train_notNA])\n",
      "    model_predictions['count'].ix[train_bools.values] = train_predictions\n",
      "    \n",
      "    # Training weekdays\n",
      "    test_bools = wkday_test_bools & (hourly_df[\"hour\"].values == i)\n",
      "    test_features = make_features(hourly_df, test_bools)\n",
      "    test_notNA = ~np.isnan(test_features).any(axis=1)\n",
      "    test_predictions = np.zeros(np.sum(test_bools))\n",
      "    test_predictions.fill(np.nan)\n",
      "    if LOG:\n",
      "        test_predictions[test_notNA] = 10**wkday_models[i].predict(test_features[test_notNA])\n",
      "    else:\n",
      "        test_predictions[test_notNA] = wkday_models[i].predict(test_features[test_notNA])\n",
      "    model_predictions['count'].ix[test_bools.values] = test_predictions\n",
      "    \n",
      "    # Training weekends\n",
      "    test_bools = wkend_test_bools & (hourly_df[\"hour\"].values == i)\n",
      "    test_features = make_features(hourly_df, test_bools)\n",
      "    test_notNA = ~np.isnan(test_features).any(axis=1)\n",
      "    test_predictions = np.zeros(np.sum(test_bools))\n",
      "    test_predictions.fill(np.nan)\n",
      "    if LOG:\n",
      "        test_predictions[test_notNA] = 10**wkend_models[i].predict(test_features[test_notNA])\n",
      "    else:\n",
      "        test_predictions[test_notNA] = wkend_models[i].predict(test_features[test_notNA])\n",
      "    model_predictions['count'].ix[test_bools.values] = test_predictions\n",
      "\n",
      "model_predictions['count'].ix[model_predictions['count'] < 0] = np.nan\n",
      "\n",
      "\n",
      "# for i in range(len(wkday_models)):\n",
      "#     print wkday_models[i].alpha_\n",
      "\n",
      "train_R2 = hourly_df['count'].ix[wkday_train_bools | wkend_train_bools].corr( \\\n",
      "    model_predictions['count'].ix[wkday_train_bools | wkend_train_bools])**2\n",
      "test_R2 = hourly_df['count'].ix[wkday_test_bools | wkend_test_bools].corr( \\\n",
      "    model_predictions['count'].ix[wkday_test_bools | wkend_test_bools])**2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}